# CVPR 2021 - Positive Sample Propagation along the Audio-Visual Event Line - Zhou et al. 

Paper: https://arxiv.org/pdf/2104.00239.pdf

## Methodogy Summary

1) Positive Sample Propagation - From the audio/visual feature encoder derive the features (A/V Initial), transform them using a linear layer (A/V Att) and calculate their scaled dot product similarity. Given N segments in a video, this will give an NxN similarity matrix. For an audio-visual event, since the event has to be present in both the modalities, only strongly correlated (and hence larger inter-modal similarity) segments in both modalities must indicate the presence of an event. To the NxN matrix, apply RELU to eliminate negatively correlated segments, normalize (L1) the values along each row and approximate the values according to the following scheme:
  a) Subtract a "min" threshold from each of the values.
  b) If the result is above 0, approximate it to 1, below or 0, equate it to 0.
Normalize (L1) the result row-wise again. Let the result NxN matrix be called SimMat. If each row in SimMat implies single audio segment to N visual segment similarity, then SimMat transpose would mean the opposite.

2) Online Feature Aggregation - Apply a separate transformation on A/V Initial called A/V Transform and achieve cross modal selection by multiplying SimMat with V Initial. By adding the A Initial as a residual, we get the visual-guided audio attention. Perform the opposite with SimMat transpose to get audio-guided visual attention.

3) A-V Pair Similarity Loss - For a sequence, let G represent the indicator vector of whether the segment at the ith position is a AVE or not. Take the aggregated features (Nxd) calculated above, perform an element-wise dot product to get a Nx1 similarity vector and normalize this vector by the L1 norm to get a prediction indicator vector S. AVPS Loss is MSE(G, S) to enforce feature alignment for AVEs.

4) For the weakly supervised setting, perform an extra FC transformation to handle the weakly supervised setting complexity. Call the output O, use another FC with sigmoid operation to determine the "weight" of each segment. Call the output WS. Scale each of the N segments of O with the corresponding output of WS and sum to get the final video level prediction.


## Observations

1) The methods used in this paper seems very re-hashed of other prior works. The PSP module is incrementally novel, the online feature aggregation is something which all prior works perform under the terminology of feature fusion and the AVPS loss seems to have the exact same motivation as the PSP module. The weighting branch in the weakly supervised setting seems incremental as well and is not novel since it is equivalent to a learned weighted MIL pooling technique.
2) Their experimentation section is a bit lacklustre with many sentences "reading off the tables" stating deltas numbers between rows. Many sentences are left uncorroborated without any proof or reference (e.g. justification for adding another FC layer for weakly supervised, their qualitative analysis sections).
