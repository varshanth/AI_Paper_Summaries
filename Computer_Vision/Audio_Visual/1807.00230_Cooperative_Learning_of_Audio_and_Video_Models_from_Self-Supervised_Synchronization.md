# NeurIPS2018 - Cooperative Learning of Audio and Video Models from Self-Supervised Synchronization, Korbar et. al.

Paper: https://arxiv.org/abs/1807.00230  

## Method Summary
### Problem Definition
- Define Audio Visual Temporal Synchronization (AVTS) problem as the task of finding out whether a given audio clip and visual clip are in sync or not
- Different from AV Correspondence in which the task is to find out whether the audio and visual clips are from the same video or not. The AVC task enables the network to capture semantic information while the AVTS task forces the network to temporal sensitive features to recognize synchronization (and hence implicitly correspondence).

### Loss function
- Natural choice to treat as binary classification problem with BCE loss initially avoided since they found it difficult to achieve convergence.
- Authors use margin loss on the features to reduce the distance between the audio and visual features of positive samples and increase the distance between the negative samples.
- AVTS Predictions made by thresholding on the distance between the audio and visual features.
- The model can then be fine-tuned later with BCE loss

### Selection of hard samples
- Positive sample: audio+visual are in sync, Easy Negative: audio and visual clips are taken from different videos, Hard Negative: audio and visual clips are taken from different time points from the same video, Super-Hard Negatives: audio and visual clip are from overlapping portions in the same video

### Curriculum Learning
- Authors found that training with just hard & super hard samples suffered degraded performance. Even 75\% easy samples + 25\% hard samples performance was bad.
- Authors employed curriculum learning wherein they trained with easy samples for the first 50 epochs and then trained with 75-25 split for the next 40 epochs and got massive gains.
- Training with super hard samples at any stage yielded worse performance
- Curriculum of learning should reflect test set statistics

### Network
- Used 3D convs in the initial layers (MCx models) and then used 2D convs for visual processing. MC3 was found the best. Audio model was VGGish on spectrograms.

### Experiments
1) Action recognitiion: Pretraining performance > Training from scratch. Pretraining with self-supervised was slightly less than fully supervised. Pretraining datasets were AudioSet, Kinetics and Imagenet.  
2) AVTS: Fine tuning with BCE loss after contrastive loss yielded better performance on test set consisting of the 75-25 easy-hard negative sample split.
3) Audio Classification: Self supervision through AVTS on SoundNet dataset with their audio subnetwork and then using a 1vsAll SVM yielded better results than L<sup>3</sup> net self supervised on SoundNet dataset.

## Observations
1) Very important takeaway is the the curriculum learning training method
2) Contrastive loss converges better than BCE loss (in this context). We can always fine tune with an auxillary loss (BCE) later.
3) Hard and extremely hard samples can hamper performance. Better to try its involvement after dealing with easy samples first.
