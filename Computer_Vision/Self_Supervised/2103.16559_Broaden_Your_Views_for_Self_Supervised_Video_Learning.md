# Broaden Your Views for Self-Supervised Video Learning - Recasens et al.
Paper Link: https://arxiv.org/pdf/2103.16559.pdf

## Methodology Summary
1) Defined 2 views for a video: Narrow View = Spatially high resolution, temporally lower resolution. Broad View = Spatially lower resolution & (Multi-modal), temporally higher resolution.
2) The narrow view may vary between 1-3 seconds and the broader view may vary between 5-10 seconds. Broader view maybe in or out of sync with the narrow view (may have different start times).
3) Defined 2 networks to process each view separately. Each network has a backbone network (direct feature extractor), a projection network (projects backbone features to latent space) and a predictor network (another projector network). Say the output of the predictor network is p.
4) Supervision is through 2 separate L2 losses. The first loss minimizes the L2 distance between the L2 normalized p_narrow and L2 normalized p_broad and the second loss vice versa. To avoid a trivial loss collapse where the networks learn to produce a constant (hence L2 dist = 0), in each of the losses, a stop-gradient operator is applied to one of the features.
5) The narrow view network would be used for downstream tasks, so the authors concentrate on it's supervision primarily. To handle "K" broad views, the narrow view network uses the same backbone network, but different projection and predictor networks while the broad view network uses different sub networks for each broad view. The losses are summed up across all the K views for each video.
6) They used different transformations as augmentation for narrow and broad views. They postulate that using different transformations will enrich the learned representations. They use a recently introduced augmentation mechanism called random convolutions (Robust and generalizable visual representation learning via random convolutions. - Xu et al.) only on the broad view. Other transformations include random cropping, horizontal flipping, scale, color jittering and gaussian blurring.
7) They use different modalities in the broad view like optical flow and audio. Audio input is in the form of spectrograms and are in sync with the narrow view (unlike the visual features).

## Experiments
1) Evalution on action recognition datasets HMDB51 and UCF101, AudioSet and ESC-50 for audio, while pretraining on Kinetics 600 for RGB & Flow and AudioSet for audio
2) 2 standard settings of evaluation: Linear setting = train a single layer or an SVM on the features output by the frozen BraVE network. Fine-tune setting = introduce a linear layer at the end, and initialize the narrow network with the pretrained (self-supervised) weights and fine tune the network end to end.

### Experimental Observations
1) Best performance when narrow view = 1.3 seconds and broad view = 10 seconds. Performance linearly increases as the broad view becomes broader and is the worst when broad view length = narrow view length.
2) Performance order: RGB < RGB + Flow << RGB + Audio < RGB + Flow + Audio
3) Performance order: Fine Tuning > Linear
4) Out of sync performance >> In sync performance: Authors hypothesize that with in-sync narrow and broad views, the broad view network discards the additional context information and just converges to an inferior narrow view representation without infusing anything to the narrow view network
5) Separate backbone, projection and predictor networks performs significantly better than sharing sub-networks.
