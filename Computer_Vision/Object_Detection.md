### Revisiting RCNN: On Awakening the Classification Power of Faster RCNN : https://arxiv.org/pdf/1803.06799.pdf 
### Decoupled Classification Refinement: Hard False Positive Suppression for Object Detection: https://arxiv.org/abs/1810.04002.pdf
  * 2 stage object detectors use feature extraction backbones which are pretrained on classification tasks. Classification tasks rely on object/objects/scenes presence and not on positional information. Hence classification tasks are translation invariant and object detection is translation covariant. A direct consequence of this is that ROIs that are part of an object, the classification head successfully tries to classify the object even though the ROI might not encapsulate the entire image. In such a case, the classifier overpowers the localization task.
  * Hard false positives do not occur due to localization but rather due to misclassification.
  * The above 2 points infer that 1) Localization and Classification heads should not share a lot of features. They should rather have their own branches (after a small common feature extraction trunk) to learn parameters to do the respective tasks separately 2) Multi-task learning through backpropagating through 2 loss functions are not effective as it may lead to sub optimal performance on both the tasks
  * Authors feed FG, BG and hard false positive crops (misclassified positives with confidence >= 0.3) i.e. object crops output from a detector, uniformly (not in some ratio like FG:BG::1:3) to train a separate classifier network (Decoupled Classification Refinement [DCR]). The class probability distribution output from this classifier is multipled by the original object detector's class probability distribution gives the final distribution. The object detector is trained first with the normal loc+cls loss and the classifier is trained next with the cls loss.
  * In the second paper, the authors claim that sampling all false positives with conf >= 0.3 leads to larger number of batches for training the classifier. Hence they introduce a sampling strategy called "top-p-sampling" wherein they sort the high confidence detections in descending order and only select those false positives which are in the top p%. High confidence false positives have higher impact than lower confidence ones. This sampling method reduced the performance by 0.2% but decreased the runtime by nearly 50%
  * The authors show that the threshold of the FP confidence does not make significant difference implying simple identification of false positives is sufficient for improvement
  * Such "decoupling" of the classification reduces the confidence of the FP and the mAP on VOC and COCO datasets.
  * Authors also show that the improvement is independent of any parameter additions - They find that a separate (Resnet151 BackBone Faster RCNN + Resnet50 Classifier) trained with the methodology performs better than just a ResNet152 BackBone Faster RCNN.
  * In the second paper, the authors show that by branching off the DCR at earlier stages from the common feature backbone, they see larger gains i.e. a longer common feature backbone hurts the mAP.
