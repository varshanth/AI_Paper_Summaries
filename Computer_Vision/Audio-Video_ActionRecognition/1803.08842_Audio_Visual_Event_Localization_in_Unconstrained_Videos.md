### ECCV18: Audio-Visual Event Localization in Unconstrained Videos, Tian et. al:
* Paper: https://arxiv.org/abs/1803.08842
* Code: https://github.com/YapengTian/AVE-ECCV18

* Audio-Visual Event is an event that is both visible and audible in a video segment
* Task is event localization. Given a video sequence of T overlapping segments each with an audio and video content, the task is to predict the event label for all the segments
* Supervised tasks are event localization in audio space alone, video space alone and cross modality localization (Given audio/visual track, predict the localization of the event in visual/audio track)
* Weakly supervised task is to predict the segment level labels during testing when the model is trained with only the video level event tag
* AVE Dataset consists of 4143 videos covering 28 categories (+1 added BG category), each of 10 seconds. AVE is derived from Google's AudioSet dataset. Annotators labelled segments which contained at least 2 second long events in which the audio and video corresponding to the event were present. 
* Audio is converted into image form i.e. log-mel spectrograms wherein the Y axis represents frequency and X axis represents time

#### Methodology for Supervised Task

![alt text](Images/1803_08842_Supervised.PNG?raw=true "Model for Supervised AVE Localization")

#### Methodology for Weakly Supervised Task

* Only video level event labels are available
* The method employed is to average pool (Reported in Paper) / max pool (Used in code) the FC outputs for each 1 second. In particular, if the FC output in the above network is of dimensions (N, t, E) where N = batch, t = segment length in seconds and E is the number of events, apply the pooling across the second dimension to get (N, 1, E), unsqueeze the second dimension into (N, E) and apply softmax on E to get the video level prediction

#### Method for Cross-Modality Localization
![alt text](Images/1803_08842_CrossModality.PNG?raw=true "Model for Cross-Modality AVE Localization")
