### Audio-Visual Event Localization in Unconstrained Videos: https://arxiv.org/abs/1803.08842  

* Audio-Visual Event is an event that is both visible and audible in a video segment
* Task is event localization. Given a video sequence of T overlapping segments each with an audio and video content, the task is to predict the event label for all the segments
* Supervised tasks are event localization in audio space alone, video space alone and cross modality localization (Given audio/visual track, predict the localization of the event in visual/audio track)
* Weakly supervised task is to predict the segment level labels during testing when the model is trained with only the video level event tag
* AVE Dataset consists of 4143 videos covering 28 categories (+1 added BG category). Annotators labelled segments which contained at least 2 second long events in which the audio and video corresponding to the event were present. 

#### Methodology for Supervised Task

![alt text](Images/1803_08842_Supervised.PNG?raw=true "Model for Supervised AVE Localization")

#### Methodology for Weakly Supervised Task

* Only video level event labels are available
* The method employed is to average pool (Reported in Paper) / max pool (Used in code) the FC outputs for each 1 second. In particular, if the FC output in the above network is of dimensions (N, t, E) where N = batch, t = segment length in seconds and E is the number of events, apply the pooling across the second dimension to get (N, 1, E), unsqueeze the second dimension into (N, E) and apply softmax on E to get the video level prediction
