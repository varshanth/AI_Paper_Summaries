
# ACM Multimedia 2020 : Look, Listen, and Attend: Co-Attention Network for Self-Supervised Audio-Visual Representation Learning, Cheng et. al.
Paper: https://arxiv.org/pdf/2008.05789.pdf  

## Methodology Summary:
- Self-supervision and representation achieved through the task of Audio-Visual Synchronization. Sample audio and visual segments starting from the same timestamp yields positive label while sampling from different timestamps yields negative label.
- Convolutional audio and visual encoders are employed to learn initial feature representations for "N" temporal samples
- These features are then passed onto self-attention (intra-modality attention) and cross-modality attention blocks (one modality features act as queries to the other modality's keys and values) for feature refinement.
- Refined modality features for N samples are flattened, concatenated and passed to an FC layer to yield synchronization probability.

## Datasets & Experiments:
- AVS task - AudioSet -> Train from scratch
- Sound Source Localization - Kinetics Sounds -> Pretrained from AVS Task
- Action Recognition - UCF101 & HMDB51 -> Pretrained from AVS Task

## Observations:
- Sharp CAMs obtained - possibly sharpened or due to thresholding of activation values?
- Borrow similar idea from ViLBERT paper wherein images (region wise) and caption features are cross refined by transforming one modality into keys and values for the other modality to query
